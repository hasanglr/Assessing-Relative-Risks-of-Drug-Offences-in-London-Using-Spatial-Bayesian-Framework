
```{r}

#import necessary libraries for this project

library("sf")
library("tmap")
library("spdep")
library("rstan")
library("geostan")
library("SpatialEpi")
library("tidybayes")
library("tidyverse")
library("here")
library(janitor) #library for rearranging the columns of the dataset
library("dplyr")
library(readxl) #for impoorting excel files



```

```{r}

#use multiple cores for parallel processing (important for Stan)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)


```

## 1. Data Importing

### 1.1. Importing Crime Data from 2010 to 2021

```{r}

# load in crime data in London at ward level

ward_crime <- read.csv(here::here("Data/MPS Ward Level Crime (Historical).csv"))%>%
  

  ##filter only drug offences
  
  dplyr::filter(MajorText =="Drug Offences") %>%
  
  ##rearrange the column names
  
  clean_names() 


```

### 1.2. Importing Crime Data from 2021 to 2023 February

```{r}


# load in crime data in London

ward_lastyear <- read.csv(here::here("Data/MPS Ward Level Crime (most recent 24 months).csv")) %>%


  ##filter only drug offences
  
  dplyr::filter(MajorText =="Drug Offences") %>%
  
  ##rearrange the column names
  
  clean_names() 

#we do not need data for year 2023 since the analysis will be conducted between 2012 and 2022.

ward_lastyear <- ward_lastyear[, -c(27:29)]


```

### 1.3. Importing Population Density (Census 2021) Data

```{r}

# load in population density

pop_den <- read.csv(here::here("Data/Census Data/TS006-2021-3-filtered-2023-04-15T15_55_08Z.csv")) %>%
  
    rename("population_density" = "Observation")

```

### 1.4. Importing Migrant Indicator (Census 2021) Data

```{r}

# load in migrant indicator data

migrant <- read.csv(here::here("Data/Census Data/TS019-2021-2-filtered-2023-04-15T17_24_47Z.csv")) %>%
  
  clean_names() %>%
  
  ##filter those who moved outside from the UK in the last year
  
  dplyr::filter(migrant_indicator_5_categories_code ==3) %>%
  
  ##change the name of column
  
  rename("migrant_outside_number" = "observation")

```

### 1.5. Importing Qualification (Census 2021) Data

```{r}

# load qualification data

qualification <- read.csv(here::here("Data/Census Data/TS067-2021-2-filtered-2023-04-15T19_56_17Z.csv")) %>%
  
  clean_names() %>%
  
  #filter those who do not have any qualification
  
  dplyr::filter(highest_level_of_qualification_8_categories_code== 0) %>%
  
  #change the name of column
  
  rename("no_qualification_number" = "observation")

```

### 1.6. Importing Ward and Borough Shapefiles

```{r}

# load the shape files

london_wards <- read_sf(here::here("Data/Wards_December_2022_Boundaries_GB_BGC_-6787463133509213153/WD_DEC_22_GB_BGC.shp"))



london_boroughs <- read_sf(here::here("Data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp"))


```

## 2. Data Wrangling

### 2.1. Wrangling the Historical Drug Offence Data

```{r}

# identify columns to remove
cols_to_remove <- grep("2010|2011", colnames(ward_crime))

# remove identified columns
ward_crime <- ward_crime[, -cols_to_remove]

```

#### 2.1.1. Sum all the months from 2012 to 2021

```{r}

# calculate row sums and add as new column

ward_crime$sum2012_2021 <- apply(ward_crime[, 6:114], 1, sum)
  

```

```{r}

    ##select only necessary columns
  
ward_crime <- ward_crime[, c("ward_name", "ward_code","minor_text","look_up_borough_name", "sum2012_2021")]

```

#### 2.1.2. Summing Drug Possession and Drug Trafficking into one discete number

```{r}

ward_crime_group <- ward_crime %>%
  group_by(ward_code,ward_name) %>%
  summarize(summed_sexual_2012_2021 = sum(sum2012_2021))


```

### 2.2. Wrangling The last 24 month Drug Offence Data

```{r}

# calculate row sums and add as new column

ward_lastyear$sum2021_2022 <- apply(ward_lastyear[, 6:26], 1, sum)

```

```{r}

    ##select only necessary columns
  
ward_lastyear<- ward_lastyear[, c("ward_name", "ward_code","minor_text","sum2021_2022")]

```

#### 2.2.1. Summing Drug Possession and Drug Trafficking into one discete number

```{r}

ward_lastyear_group <- ward_lastyear %>%
  group_by(ward_code,ward_name) %>%
  summarize(summed_sexual_2021_2022 = sum(sum2021_2022))


```

### 2.3. Joining ward spatial data with drug offence data

```{r}

#left join

ward_joined <- london_wards %>%
  

  
  left_join(., 
            ward_crime_group, by=c("WD22CD"="ward_code")) %>%
  
    
  left_join(., 
            ward_lastyear_group, by=c("WD22CD"="ward_code"))

```

```{r}

#only selecting London wards

ward_joined <- ward_joined[complete.cases(ward_joined$summed_sexual_2012_2021),]


```

```{r}

#summing 2012-2021 data and 2021-2022 data

ward_joined$total_2012_2022 <- ward_joined$summed_sexual_2012_2021 + ward_joined$summed_sexual_2021_2022

```

```{r}

#left join

final_dataset <- ward_joined %>%
  
  ##join with population density
  left_join(., 
            pop_den, by=c("WD22CD"="Electoral.wards.and.divisions.Code")) %>%
  
  ##join with migrant data
  
    left_join(., 
            migrant, by=c("WD22CD"="electoral_wards_and_divisions_code")) %>%
  
  ##join with quantification data
  
    left_join(., 
            qualification, by=c("WD22CD"="electoral_wards_and_divisions_code"))  %>%
  
  ##select only necessary columns for the analysis
  
   dplyr::select("WD22CD",
                "WD22NM",
                "geometry",
                "total_2012_2022",
                "no_qualification_number",
                "migrant_outside_number",
                "population_density")

```

### 2.4. Calculating the population for expected number used in the model

```{r}

#calculate the area of the wards

final_dataset$area <- st_area(final_dataset)

```

```{r}

#calculte the population from multyipling population density and area 

final_dataset$areas_km2 <- final_dataset$area / 1e+06

final_dataset$population <- final_dataset$areas_km2 * final_dataset$population_density

final_dataset$population <- as.numeric(final_dataset$population)

```

#### 2.4.1 Calculating the expected number

```{r}

# calculate the expected number of cases
final_dataset$ExpectedNum <- round(expected(population = final_dataset$population, cases = final_dataset$total_2012_2022, n.strata = 1), 0)

```



## 3. Mapping the number of drug offence

```{r}

#look at the crs of our data

st_crs(final_dataset)

```

```{r}
#mapping the distribution of drug offences 

tm_shape(final_dataset)+
  tm_fill("total_2012_2022",
          style="jenks",
          n= 7, 
          palette = "OrRd",
          title= "The Number of Offences")+
  tm_borders(col = "grey", lwd=0.22) +
  
  ##london boroughs
  
  tm_shape(london_boroughs) +
  tm_borders(col = "black", lwd=0.3) +

  
  ##north arrow
  
  tm_compass(north=0,
             position=c(0.87,0.35),size=1.3, show.labels= 0)+
  
  ##scale bar
  
  tm_scale_bar(position = c(0.003,0.0001),text.size =0.55) +
  
  ##legend position
  
  tm_layout(legend.position = c(0.789,0.05),
          legend.title.size = 0.76,
          legend.text.size = 0.60) +
  
  ##title
  
   tm_credits("a) Drug Offences across London Wards (Natural Breaks)", position=c(0.01,0.94), size=1.05, fontface = "bold") +
   tm_credits("Source: Office for National Statistics licensed under the Open Government Licence v.1.0.", position=c(0.01,0.084), size=0.35)+
  tm_credits("Contains National Statistics data © Crown copyright and database right [2022]",position=c(0.01,0.064), size=0.35)+
  tm_credits("Contains Ordnance Survey data © Crown copyright and database right [2022]",position=c(0.01,0.044), size=0.35) 

##save
tmap_save()

```

#### 3.1. Spatial Autocorrelation (Global Moran's I) of Drug Offences in London

Note about codes used in this part

The most of scripts used in this part are adapted from practical sessions of GEOG0114 Principles of Spatial Analysis in Social and Geographic Data Science taught at UCL created by Anwar Musah.

```{r}

#calculate the centroids

centroids<- final_dataset%>%
  st_centroid()%>%
  st_geometry()

#find four k-nearest neighbours

knn <-centroids %>%
  knearneigh(., k=4)
knn2 <- knn%>%
  knn2nb()

#calculate the weights

knn3_weight <- knn2 %>%
  nb2listw(., style="W")

#Global Moran's I on the number of drug offences in London Wards

moran.test(final_dataset$total_2012_2022, knn3_weight)
```

#### 3.2. Local Indıcators of Spatial Autocorrelation (LISA)

```{r}

#convert the dataset into sp

final_dataset_sp <- as_Spatial(final_dataset)

#LISA (Local Moran's I) 

local_moran_density <- localmoran(final_dataset_sp$total_2012_2022, knn3_weight)


final_dataset_sp$total_2012_2022_scale <- scale(final_dataset_sp$total_2012_2022)

# create a spatial lag variable 

final_dataset_sp$total_2012_2022_lag <- lag.listw(knn3_weight,final_dataset_sp$total_2012_2022)

# convert the sp into sf again
final_dataset_sf<- st_as_sf(final_dataset_sp)

# set a significance value

sig_level <- 0.1

# classification with significance value

final_dataset_sf$quad_sig <- ifelse(final_dataset_sf$total_2012_2022_scale > 0 & 
                                          final_dataset_sf$total_2012_2022_lag > 0 & 
                                          local_moran_density[,5] <= sig_level, 
                                          'High-high (Hot spots)', 
                                   ifelse(final_dataset_sf$total_2012_2022_scale <= 0 & 
                                          final_dataset_sf$total_2012_2022_lag <= 0 & 
                                          local_moran_density[,5] <= sig_level, 
                                          'Low-Low (Cold Spots)', 
                                   ifelse(final_dataset_sf$total_2012_2022_scale > 0 & 
                                          final_dataset_sf$total_2012_2022_lag <= 0 & 
                                          local_moran_density[,5] <= sig_level, 
                                          'High-Low (Outliers)', 
                                   ifelse(final_dataset_sf$total_2012_2022_scale <= 0 & 
                                          final_dataset_sf$total_2012_2022_lag> 0 & 
                                          local_moran_density[,5] <= sig_level, 
                                          'Low-High (Outliers)',
                                   ifelse(local_moran_density[,5] > sig_level, 
                                          'Not-significant', 
                                          'Not-significant')))))
```


```{r}

#convert the sf into data frame

final_dataset_df<- as.data.frame(final_dataset_sf) %>%
  
  ##select necessary columns
  
    dplyr::select("quad_sig",
                "WD22CD")
#join 
final_dataset_sf_lisa <- final_dataset %>%
  left_join(.,
            final_dataset_df, by = c("WD22CD" = "WD22CD"))
```

```{r}

#mapping significant clusters

tm_shape(final_dataset_sf_lisa) +
  
  tm_fill(col = 'quad_sig', 
            palette = c("#e34a33", "#bdd7e7", "white",  "#2c7fb8"),
            title= "Clusters") +
  tm_borders(col = "grey", lwd=0.22) +
  
  ##london boroughs
  
    tm_shape(london_boroughs) +
  tm_borders(col = "black", lwd=0.3) +

  
  ##north arrow
  
  tm_compass(north=0,
             position=c(0.87,0.35),size=1.3, show.labels= 0)+
  
  ##scale bar
  
  tm_scale_bar(position = c(0.003,0.0001),text.size =0.35) +
  
  ##legend position
  
  tm_layout(legend.position = c(0.789,0.05),
          legend.title.size = 0.76,
          legend.text.size = 0.60) +
  
  ##title
  
   tm_credits("b) Statistically Significant Clusters of Drug Offences)", position=c(0.01,0.94), size=1.05, fontface = "bold") +
   tm_credits("Source: Office for National Statistics licensed under the Open Government Licence v.1.0.", position=c(0.01,0.084), size=0.35)+
  tm_credits("Contains National Statistics data © Crown copyright and database right [2022]",position=c(0.01,0.064), size=0.35)+
  tm_credits("Contains Ordnance Survey data © Crown copyright and database right [2022]",position=c(0.01,0.044), size=0.35) 

##save
tmap_save()


```
## 4. Spatial intrinsic conditional autoregressive model (ICAR)

### 4.1. Creating Adjacency Matrix

```{r}

# need to be coerced into a spatial object

sp.object <- as(final_dataset, "Spatial")

```

```{r}

# needs to be coerced into a matrix object

adjacencyMatrix <- shape2mat(sp.object)

# we extract the components for the ICAR model
extractComponents <- prep_icar_data(adjacencyMatrix)

```

```{r}

#calculte nodes and edges

n <- as.numeric(extractComponents$group_size)
nod1 <- extractComponents$node1
nod2 <- extractComponents$node2
n_edges <- as.numeric(extractComponents$n_edges)

```

### 4.2. Identifying Dependent and Independent Variables


```{r}

#drop geometry column to use as independent variables

final1 <- st_drop_geometry(final_dataset)
```

```{r}

#identifty dependent, independent variables and expected number

##dependent variable

y <- final_dataset$total_2012_2022

##independent variables as a matrix

x <- as.matrix(final1[, c("no_qualification_number","migrant_outside_number")])

##expected number

e <- final_dataset$ExpectedNum

```

### 4.3. Model with Stan


```{r}

# put all components into a list object
stan.spatial.dataset <- list(N=n, N_edges=n_edges, node1=nod1, node2=nod2, Y=y, X=x, E=e)

```

```{r}

#fit the model with stan code

icar_poisson_fit = stan("coursework.stan", data=stan.spatial.dataset, iter=20000, chains=6, verbose = FALSE)

```

### 4.4. Printing of the global results

```{r}

# remove that annoying scientific notation
options(scipen = 999)
summary(icar_poisson_fit, pars=c("alpha", "beta", "sigma"), probs=c(0.025, 0.975))$summary

```

```{r}

# show first 6 rows only

head(summary(icar_poisson_fit, pars=c("phi"), probs=c(0.025, 0.975))$summary)


```

```{r}

#print alpha, beta, sigma and phi

print(icar_poisson_fit, pars=c("alpha", "beta", "sigma", "phi"), probs=c(0.025, 0.975))

```
### 4.5. Rapid diagnostics of the rHATs

```{r}

# diagnostic check on the rHats - put everything into a data frame
diagnostic.checks <- as.data.frame(summary(icar_poisson_fit, pars=c("alpha", "beta", "sigma", "phi", "lp__"), probs=c(0.025, 0.5, 0.975))$summary)
# create binary variable
diagnostic.checks$valid <- ifelse(diagnostic.checks$Rhat < 1.1, 1, 0)
# tabulate it
table(diagnostic.checks$valid)

```

All outputted parameters have an rHAT < 1.1


### 4.6. Creating Area-specific Relative Risk Map

```{r}

# extraction key posterior results for the generated quantities 
relativeRisk.results <- as.data.frame(summary(icar_poisson_fit, pars=c("mu"), probs=c(0.025, 0.975))$summary)

# now cleaning up this table up
# first, insert clean row numbers to new data frame

row.names(relativeRisk.results) <- 1:nrow(relativeRisk.results)

# second, rearrange the columns into order
relativeRisk.results <- relativeRisk.results[, c(1,4,5,7)]

# third, rename the columns appropriately
colnames(relativeRisk.results)[1] <- "rr"
colnames(relativeRisk.results)[2] <- "rrlower"
colnames(relativeRisk.results)[3] <- "rrupper"
colnames(relativeRisk.results)[4] <- "rHAT"

# view clean table 
head(relativeRisk.results)

```

```{r}

# now, we proceed to generate our risk maps
# align the results to the areas in shapefile
final_dataset$rr <- relativeRisk.results[, "rr"]
final_dataset$rrlower <- relativeRisk.results[, "rrlower"]
final_dataset$rrupper <- relativeRisk.results[, "rrupper"]


```

```{r}

# create categories to define if an area has significant increase or decrease in risk, or nothing all 

final_dataset$Significance <- NA
final_dataset$Significance[final_dataset$rrlower<1 & final_dataset$rrupper>1] <- 0    # NOT SIGNIFICANT
final_dataset$Significance[final_dataset$rrlower==1 | final_dataset$rrupper==1] <- 0  # NOT SIGNIFICANT
final_dataset$Significance[final_dataset$rrlower>1 & final_dataset$rrupper>1] <- 1    # SIGNIFICANT INCREASE
final_dataset$Significance[final_dataset$rrlower<1 & final_dataset$rrupper<1] <- -1   # SIGNIFICANT DECREASE


```

```{r}

# For map design for the relative risk -- you want to understand or get a handle on what the distribution for risks look like
# this would inform you of how to create the labelling for the legends when make a map in tmap
summary(final_dataset$rr)
hist(final_dataset$rr)

```

```{r}

# creating the labels
RiskCategorylist <- c(">0.0 to 0.25", "0.26 to 0.50", "0.51 to 0.75", "0.76 to 0.99", "1.00 & <1.01",
    "1.01 to 1.10", "1.11 to 1.25", "1.26 to 1.50", "1.51 to 1.75", "1.76 to 2.00", "2.01 to 3.00")

```

```{r}

# categorising the risk values to match the labelling in RiskCategorylist object
final_dataset$RelativeRiskCat <- NA
final_dataset$RelativeRiskCat[final_dataset$rr>= 0 & final_dataset$rr <= 0.25] <- -4
final_dataset$RelativeRiskCat[final_dataset$rr> 0.25 & final_dataset$rr <= 0.50] <- -3
final_dataset$RelativeRiskCat[final_dataset$rr> 0.50 & final_dataset$rr <= 0.75] <- -2
final_dataset$RelativeRiskCat[final_dataset$rr> 0.75 & final_dataset$rr < 1] <- -1
final_dataset$RelativeRiskCat[final_dataset$rr>= 1.00 & final_dataset$rr < 1.01] <- 0
final_dataset$RelativeRiskCat[final_dataset$rr>= 1.01 & final_dataset$rr <= 1.10] <- 1
final_dataset$RelativeRiskCat[final_dataset$rr> 1.10 & final_dataset$rr <= 1.25] <- 2
final_dataset$RelativeRiskCat[final_dataset$rr> 1.25 & final_dataset$rr <= 1.50] <- 3
final_dataset$RelativeRiskCat[final_dataset$rr> 1.50 & final_dataset$rr <= 1.75] <- 4
final_dataset$RelativeRiskCat[final_dataset$rr> 1.75 & final_dataset$rr <= 2.00] <- 5
final_dataset$RelativeRiskCat[final_dataset$rr> 2.00 & final_dataset$rr <= 10] <- 6

# check to see if legend scheme is balanced - if a number is missing that categorisation is wrong!
table(final_dataset$RelativeRiskCat)
```

```{r}
#mapping relative risks

tm_shape(final_dataset)+
  tm_fill("RelativeRiskCat",
          style="cat",
          labels = RiskCategorylist,
          palette = "-RdBu",
          title= "Relative Risk")+
    tm_borders(col = "white", lwd=0.087) +
  
  ##london boroughs
  tm_shape(london_boroughs) +
  tm_borders(col = "black", lwd=1.2) +

  ##north arrow
  
  tm_compass(north=0,
             position=c(0.10,0.20),size=1.3, show.labels= 0)+
  
  ##scale bar
  
  tm_scale_bar(position = c(0.003,0.0001),text.size =0.35) +
  
  ##legend position
  
  tm_layout(legend.position = c(0.825,0.01),
          legend.title.size = 0.76,
          legend.text.size = 0.60) +
  
  
  ##title
  
   tm_credits("a) Relative Risks", position=c(0.01,0.94), size=1.05, fontface = "bold") +
  tm_credits("Contains National Statistics data © Crown copyright and database right [2022]",position=c(0.01,0.074), size=0.35)+
  tm_credits("Contains Ordnance Survey data © Crown copyright and database right [2022]",position=c(0.01,0.054), size=0.35) 


tmap_save()



```
### 4.7. Creating Significance of Relative Risk Map

```{r}
#mapping the significance of relative risks

tm_shape(final_dataset) + 
    tm_fill("Significance", style = "cat", title = "Significance Categories", 
        palette = c("#2c7fb8","white","#e34a33"), labels = c("Significantly low", "Not Significant", "Significantly high")) +
    tm_borders(col = "white", lwd=0.070) +
  
  ##london boroughs
  tm_shape(london_boroughs) +
  tm_borders(col = "black", lwd=0.70) +
  
  ##north arrow
  
  tm_compass(north=0,
             position=c(0.10,0.20),size=1.3, show.labels= 0)+
  
  ##scale bar
  
  tm_scale_bar(position = c(0.003,0.0001),text.size =0.35) +
  
  ##legend position
  
  tm_layout(legend.position = c(0.78,0.06),
          legend.title.size = 0.76,
          legend.text.size = 0.60) +
  
  
  ##title
  
   tm_credits("a) Significance of Relative Risks", position=c(0.01,0.94), size=1.05, fontface = "bold") +
  tm_credits("Contains National Statistics data © Crown copyright and database right [2022]",position=c(0.01,0.074), size=0.35)+
  tm_credits("Contains Ordnance Survey data © Crown copyright and database right [2022]",position=c(0.01,0.054), size=0.35) 


tmap_save()

```
### 4.8. Calculating and Mapping Exceedance Probabilities

```{r}

# extract the exceedence probabilities from the icar_possion_fit object
# compute the probability that an area has a relative risk ratio > 1.0
threshold <- function(x){mean(x > 1.00)}
excProbrr <- icar_poisson_fit %>% spread_draws(mu[i]) %>% 
    group_by(i) %>% summarise(mu=threshold(mu)) %>%
    pull(mu)

# insert the exceedance values into the spatial data frame
final_dataset$excProb <- excProbrr


```

```{r}

# create the labels for the probabilities
ProbCategorylist <- c("<0.01", "0.01-0.09", "0.10-0.19", "0.20-0.29", "0.30-0.39", "0.40-0.49","0.50-0.59", "0.60-0.69", "0.70-0.79", "0.80-0.89", "0.90-0.99", "1.00")

# categorising the probabilities in bands of 10s
final_dataset$ProbCat <- NA
final_dataset$ProbCat[final_dataset$excProb>=0 & final_dataset$excProb< 0.01] <- 1
final_dataset$ProbCat[final_dataset$excProb>=0.01 & final_dataset$excProb< 0.10] <- 2
final_dataset$ProbCat[final_dataset$excProb>=0.10 & final_dataset$excProb< 0.20] <- 3
final_dataset$ProbCat[final_dataset$excProb>=0.20 & final_dataset$excProb< 0.30] <- 4
final_dataset$ProbCat[final_dataset$excProb>=0.30 & final_dataset$excProb< 0.40] <- 5
final_dataset$ProbCat[final_dataset$excProb>=0.40 & final_dataset$excProb< 0.50] <- 6
final_dataset$ProbCat[final_dataset$excProb>=0.50 & final_dataset$excProb< 0.60] <- 7
final_dataset$ProbCat[final_dataset$excProb>=0.60 & final_dataset$excProb< 0.70] <- 8
final_dataset$ProbCat[final_dataset$excProb>=0.70 & final_dataset$excProb< 0.80] <- 9
final_dataset$ProbCat[final_dataset$excProb>=0.80 & final_dataset$excProb< 0.90] <- 10
final_dataset$ProbCat[final_dataset$excProb>=0.90 & final_dataset$excProb< 1.00] <- 11
final_dataset$ProbCat[final_dataset$excProb == 1.00] <- 12

# check to see if legend scheme is balanced
table(final_dataset$ProbCat)

```

```{r}

# map of exceedance probabilities
tm_shape(final_dataset) + 
    tm_fill("ProbCat", style = "cat", title = "Probability", palette = "Reds", labels = ProbCategorylist) +
    tm_borders(col = "#928B8B", lwd=0.195) +

    ##london boroughs
     tm_shape(london_boroughs) +
     tm_borders(col = "black", lwd=0.5) +


  ##north arrow
  
  tm_compass(north=0,
             position=c(0.10,0.20),size=1.3, show.labels= 0)+
  
  ##scale bar
  
  tm_scale_bar(position = c(0.003,0.0001),text.size =0.35) +
  ##legend position
  
  tm_layout(legend.position = c(0.84,0.05),
          legend.title.size = 0.76,
          legend.text.size = 0.60) +
  
  
  ##title
  
   tm_credits("Exceedance Probabilities", position=c(0.01,0.94), size=1.05, fontface = "bold") +
  tm_credits("Contains National Statistics data © Crown copyright and database right [2022]",position=c(0.01,0.074), size=0.35)+
  tm_credits("Contains Ordnance Survey data © Crown copyright and database right [2022]",position=c(0.01,0.054), size=0.35) 

##save
tmap_save()
```
