# Risk Assessment for Drug Offences in London

```{r}

#import necessary libraries for this project

library("sf")
library("tmap")
library("spdep")
library("rstan")
library("geostan")
library("SpatialEpi")
library("tidybayes")
library("tidyverse")
library("here")
library(janitor) #library for rearranging the columns of the dataset
library("dplyr")
library(readxl) #for impoorting excel files



```

```{r}

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)


```

## 1. DATA IMPORTING

### 1.1. Importing Crime Data from 2010 to 2021

```{r}

# load in crime data in London at ward level

ward_crime <- read.csv(here::here("Data/MPS Ward Level Crime (Historical).csv"))%>%
  

      ##filter only Drug offences
  
  dplyr::filter(MajorText =="Drug Offences") %>%
  
  ##rearrange the column names
  
  clean_names() 


```

### 1.2. Importing Crime Data from 2021 to 2023 February

```{r}


# load in crime data in London

ward_lastyear <- read.csv(here::here("Data/MPS Ward Level Crime (most recent 24 months).csv")) %>%


  ##filter only drug offences
  
  dplyr::filter(MajorText =="Drug Offences") %>%
  
  ##rearrange the column names
  
  clean_names() 

##we dont need data for 2023

ward_lastyear <- ward_lastyear[, -c(27:29)]


```

### 1.3. Importing Population Density (Census 2021) Data


```{r}


# load in population density

pop_den <- read.csv(here::here("Data/Census Data/TS006-2021-3-filtered-2023-04-15T15_55_08Z.csv")) %>%
  
    rename("population_density" = "Observation")

```


### 1.4. Importing Migrant Indicator (Census 2021) Data

```{r}

# load in migrant indicator data

migrant <- read.csv(here::here("Data/Census Data/TS019-2021-2-filtered-2023-04-15T17_24_47Z.csv")) %>%
  
  clean_names() %>%
  
  
  #filter those who moved outside from the UK in the last year
  
  dplyr::filter(migrant_indicator_5_categories_code ==3) %>%
  
  #change the name of column
  
  rename("migrant_outside_number" = "observation")

```

### 1.5. Importing Qualification (Census 2021) Data


```{r}

# load qualification data

qualification <- read.csv(here::here("Data/Census Data/TS067-2021-2-filtered-2023-04-15T19_56_17Z.csv")) %>%
  
  clean_names() %>%
  
  #filter those who do not have any qualification
  
  dplyr::filter(highest_level_of_qualification_8_categories_code== 0) %>%
  
  #change the name of column
  
  rename("no_qualification_number" = "observation")

```

### 1.6. Importing Ward and Borough Shapefiles

```{r}

# load the shape files

london_wards <- read_sf(here::here("Data/Wards_December_2022_Boundaries_GB_BGC_-6787463133509213153/WD_DEC_22_GB_BGC.shp"))



london_boroughs <- read_sf(here::here("Data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp"))


```


## 2. DATA WRANGLING

### 2.1. Wrangling the Historical Drug Offence Data

```{r}

# identify columns to remove
cols_to_remove <- grep("2010|2011", colnames(ward_crime))

# remove identified columns
ward_crime <- ward_crime[, -cols_to_remove]

```

#### 2.1.1. Sum all the months from 2012 to 2021

```{r}

# calculate row sums and add as new column

ward_crime$sum2012_2021 <- apply(ward_crime[, 6:114], 1, sum)
  

```

```{r}

    ##select only necessary columns
  
ward_crime <- ward_crime[, c("ward_name", "ward_code","minor_text","look_up_borough_name", "sum2012_2021")]

```


#### 2.1.2. Summing Drug Possession and Drug Trafficking into one discete number


```{r}

ward_crime_group <- ward_crime %>%
  group_by(ward_code,ward_name) %>%
  summarize(summed_sexual_2012_2021 = sum(sum2012_2021))


```

### 2.2. Wrangling The last 24 month Drug Offence Data

```{r}

# calculate row sums and add as new column

ward_lastyear$sum2021_2022 <- apply(ward_lastyear[, 6:26], 1, sum)

```

```{r}

    ##select only necessary columns
  
ward_lastyear<- ward_lastyear[, c("ward_name", "ward_code","minor_text","sum2021_2022")]

```


#### 2.2.1. Summing Drug Possession and Drug Trafficking into one discete number


```{r}

ward_lastyear_group <- ward_lastyear %>%
  group_by(ward_code,ward_name) %>%
  summarize(summed_sexual_2021_2022 = sum(sum2021_2022))


```

### 2.3. Joining ward spatial data with drug offence data

```{r}

#left join

ward_joined <- london_wards %>%
  

  
  left_join(., 
            ward_crime_group, by=c("WD22CD"="ward_code")) %>%
  
    
  left_join(., 
            ward_lastyear_group, by=c("WD22CD"="ward_code"))

```


```{r}

#only selecting London wards

ward_joined <- ward_joined[complete.cases(ward_joined$summed_sexual_2012_2021),]


```


```{r}

#summing 2012-2021 data and 2021-2022 data

ward_joined$total_2012_2022 <- ward_joined$summed_sexual_2012_2021 + ward_joined$summed_sexual_2021_2022

```

```{r}

#left join

final_dataset <- ward_joined %>%
  

  
  left_join(., 
            pop_den, by=c("WD22CD"="Electoral.wards.and.divisions.Code")) %>%
  
    left_join(., 
            migrant, by=c("WD22CD"="electoral_wards_and_divisions_code")) %>%
    left_join(., 
            qualification, by=c("WD22CD"="electoral_wards_and_divisions_code"))  %>%
  
   dplyr::select("WD22CD",
                "WD22NM",
                "geometry",
                "total_2012_2022",
                "no_qualification_number",
                "migrant_outside_number",
                "population_density")

```

```{r}

final_dataset$area <- st_area(final_dataset)

```

```{r}

final_dataset$areas_km2 <- final_dataset$area / 1e+06

final_dataset$population <- final_dataset$areas_km2 * final_dataset$population_density

final_dataset$population <- as.numeric(final_dataset$population)

```

## 3. Mapping the number of drug offence 

```{r}

#look at the crs of our data

st_crs(final_dataset)
```

```{r}

tm_shape(ward_joined)+
  tm_fill("total_2012_2022",
          style="jenks",
          n= 7, 
          palette = "OrRd",
          title= "The Number of Offences")+
  tm_shape(london_boroughs) +
  tm_borders(col = "black", lwd=0.3) +

  
  ##north arrow
  
  tm_compass(north=0,
             position=c(0.15,0.10),size=1.5, show.labels= 0)+
  
  ##scale bar
  
  tm_scale_bar(position = c(0.003,0.02)) +
  
  ##legend position
  
  tm_layout(legend.position = c(0.789,0.05),
          legend.title.size = 0.76,
          legend.text.size = 0.60) +
  
  ##title
  
   tm_credits("Drug Offences across London Wards (Natural Breaks)", position=c(0.20,0.94), size=0.8) 




```



```{r}

# calculate the expected number of cases
final_dataset$ExpectedNum <- round(expected(population = final_dataset$population, cases = final_dataset$total_2012_2022, n.strata = 1), 0)

```

3.  ADJACENCY MATRIX

```{r}


# need to be coerced into a spatial object
sp.object <- as(final_dataset, "Spatial")

```

```{r}

# needs to be coerced into a matrix object
adjacencyMatrix <- shape2mat(sp.object)

# we extract the components for the ICAR model
extractComponents <- prep_icar_data(adjacencyMatrix)

```

```{r}


n <- as.numeric(extractComponents$group_size)
nod1 <- extractComponents$node1
nod2 <- extractComponents$node2
n_edges <- as.numeric(extractComponents$n_edges)

```

4.  Create dataset for STAN

```{r}
final1 <- st_drop_geometry(final_dataset)
```

```{r}

y <- final_dataset$total_2012_2022
x <- as.matrix(final1[, c("no_qualification_number","migrant_outside_number")])

e <- final_dataset$ExpectedNum

```

```{r}

# put all components into a list object
stan.spatial.dataset <- list(N=n, N_edges=n_edges, node1=nod1, node2=nod2, Y=y, X=x, E=e)

```

```{r}

icar_poisson_fit = stan("coursework.stan", data=stan.spatial.dataset, iter=20000, chains=6, verbose = FALSE)

```

```{r}
# remove that annoying scientific notation
options(scipen = 999)
summary(icar_poisson_fit, pars=c("alpha", "beta", "sigma"), probs=c(0.025, 0.975))$summary
```

```{r}

# show first 6 rows only instead of the full 307
head(summary(icar_poisson_fit, pars=c("phi"), probs=c(0.025, 0.975))$summary)

```

```{r}

print(icar_poisson_fit, pars=c("alpha", "beta", "sigma", "phi"), probs=c(0.025, 0.975))

```

```{r}

# diagnostic check on the rHats - put everything into a data frame
diagnostic.checks <- as.data.frame(summary(icar_poisson_fit, pars=c("alpha", "beta", "sigma", "phi", "lp__"), probs=c(0.025, 0.5, 0.975))$summary)
# create binary variable
diagnostic.checks$valid <- ifelse(diagnostic.checks$Rhat < 1.1, 1, 0)
# tabulate it
table(diagnostic.checks$valid)

```

```{r}


# extraction key posterior results for the generated quantities 
relativeRisk.results <- as.data.frame(summary(icar_poisson_fit, pars=c("mu"), probs=c(0.025, 0.975))$summary)
# now cleaning up this table up
# first, insert clean row numbers to new data frame
row.names(relativeRisk.results) <- 1:nrow(relativeRisk.results)
# second, rearrange the columns into order
relativeRisk.results <- relativeRisk.results[, c(1,4,5,7)]
# third, rename the columns appropriately
colnames(relativeRisk.results)[1] <- "rr"
colnames(relativeRisk.results)[2] <- "rrlower"
colnames(relativeRisk.results)[3] <- "rrupper"
colnames(relativeRisk.results)[4] <- "rHAT"

# view clean table 
head(relativeRisk.results)

```

```{r}
# now, we proceed to generate our risk maps
# align the results to the areas in shapefile
final_dataset$rr <- relativeRisk.results[, "rr"]
final_dataset$rrlower <- relativeRisk.results[, "rrlower"]
final_dataset$rrupper <- relativeRisk.results[, "rrupper"]


```

```{r}
# create categories to define if an area has significant increase or decrease in risk, or nothing all 

final_dataset$Significance <- NA
final_dataset$Significance[final_dataset$rrlower<1 & final_dataset$rrupper>1] <- 0    # NOT SIGNIFICANT
final_dataset$Significance[final_dataset$rrlower==1 | final_dataset$rrupper==1] <- 0  # NOT SIGNIFICANT
final_dataset$Significance[final_dataset$rrlower>1 & final_dataset$rrupper>1] <- 1    # SIGNIFICANT INCREASE
final_dataset$Significance[final_dataset$rrlower<1 & final_dataset$rrupper<1] <- -1   # SIGNIFICANT DECREASE


```

```{r}

# For map design for the relative risk -- you want to understand or get a handle on what the distribution for risks look like
# this would inform you of how to create the labelling for the legends when make a map in tmap
summary(final_dataset$rr)
hist(final_dataset$rr)

```
```{r}

# creating the labels
RiskCategorylist <- c(">0.0 to 0.25", "0.26 to 0.50", "0.51 to 0.75", "0.76 to 0.99", "1.00 & <1.01",
    "1.01 to 1.10", "1.11 to 1.25", "1.26 to 1.50", "1.51 to 1.75", "1.76 to 2.00", "2.01 to 3.00")

# next, we are creating the discrete colour changes for my legends and want to use a divergent colour scheme
# scheme ranges from extreme dark blues to light blues to white to light reds to extreme dark reds
# you can pick your own colour choices by checking out this link [https://colorbrewer2.org]

RRPalette <- c("#65bafe","#98cffe","#cbe6fe","#dfeffe","white","#fed5d5","#fcbba1","#fc9272","#fb6a4a","#de2d26","#a50f15")

```



```{r}

# categorising the risk values to match the labelling in RiskCategorylist object
final_dataset$RelativeRiskCat <- NA
final_dataset$RelativeRiskCat[final_dataset$rr>= 0 & final_dataset$rr <= 0.25] <- -4
final_dataset$RelativeRiskCat[final_dataset$rr> 0.25 & final_dataset$rr <= 0.50] <- -3
final_dataset$RelativeRiskCat[final_dataset$rr> 0.50 & final_dataset$rr <= 0.75] <- -2
final_dataset$RelativeRiskCat[final_dataset$rr> 0.75 & final_dataset$rr < 1] <- -1
final_dataset$RelativeRiskCat[final_dataset$rr>= 1.00 & final_dataset$rr < 1.01] <- 0
final_dataset$RelativeRiskCat[final_dataset$rr>= 1.01 & final_dataset$rr <= 1.10] <- 1
final_dataset$RelativeRiskCat[final_dataset$rr> 1.10 & final_dataset$rr <= 1.25] <- 2
final_dataset$RelativeRiskCat[final_dataset$rr> 1.25 & final_dataset$rr <= 1.50] <- 3
final_dataset$RelativeRiskCat[final_dataset$rr> 1.50 & final_dataset$rr <= 1.75] <- 4
final_dataset$RelativeRiskCat[final_dataset$rr> 1.75 & final_dataset$rr <= 2.00] <- 5
final_dataset$RelativeRiskCat[final_dataset$rr> 2.00 & final_dataset$rr <= 10] <- 6

# check to see if legend scheme is balanced - if a number is missing that categorisation is wrong!
table(final_dataset$RelativeRiskCat)
```

```{r}


tm_shape(final_dataset)+
  tm_fill("RelativeRiskCat",
          style="cat",
          palette = "OrRd",
          title= "Rekative Risk")+


```
```{r}

tm_shape(final_dataset) + 
    tm_fill("Significance", style = "cat", title = "Significance Categories", 
        palette = c("#33a6fe", "white", "#fe0000"), labels = c("Significantly low", "Not Significant", "Significantly high")) +
  
    tm_shape(london_boroughs) +
  tm_borders(col = "black", lwd=0.3) +

  ##north arrow
  
  tm_compass(north=0,
             position=c(0.15,0.10),size=1.5, show.labels= 0)+
  
  ##scale bar
  
  tm_scale_bar(position = c(0.003,0.02)) +
  
  ##legend position
  
  tm_layout(legend.position = c(0.84,0.05),
          legend.title.size = 0.76,
          legend.text.size = 0.60) +
  
  ##title
  
   tm_credits("Significance", position=c(0.20,0.94), size=0.8) 


```

Extracting and mapping of the exceedance probabilities

```{r}

# extract the exceedence probabilities from the icar_possion_fit object
# compute the probability that an area has a relative risk ratio > 1.0
threshold <- function(x){mean(x > 1.00)}
excProbrr <- icar_poisson_fit %>% spread_draws(mu[i]) %>% 
    group_by(i) %>% summarise(mu=threshold(mu)) %>%
    pull(mu)

# insert the exceedance values into the spatial data frame
final_dataset$excProb <- excProbrr


```


```{r}

# create the labels for the probabilities
ProbCategorylist <- c("<0.01", "0.01-0.09", "0.10-0.19", "0.20-0.29", "0.30-0.39", "0.40-0.49","0.50-0.59", "0.60-0.69", "0.70-0.79", "0.80-0.89", "0.90-0.99", "1.00")

# categorising the probabilities in bands of 10s
final_dataset$ProbCat <- NA
final_dataset$ProbCat[final_dataset$excProb>=0 & final_dataset$excProb< 0.01] <- 1
final_dataset$ProbCat[final_dataset$excProb>=0.01 & final_dataset$excProb< 0.10] <- 2
final_dataset$ProbCat[final_dataset$excProb>=0.10 & final_dataset$excProb< 0.20] <- 3
final_dataset$ProbCat[final_dataset$excProb>=0.20 & final_dataset$excProb< 0.30] <- 4
final_dataset$ProbCat[final_dataset$excProb>=0.30 & final_dataset$excProb< 0.40] <- 5
final_dataset$ProbCat[final_dataset$excProb>=0.40 & final_dataset$excProb< 0.50] <- 6
final_dataset$ProbCat[final_dataset$excProb>=0.50 & final_dataset$excProb< 0.60] <- 7
final_dataset$ProbCat[final_dataset$excProb>=0.60 & final_dataset$excProb< 0.70] <- 8
final_dataset$ProbCat[final_dataset$excProb>=0.70 & final_dataset$excProb< 0.80] <- 9
final_dataset$ProbCat[final_dataset$excProb>=0.80 & final_dataset$excProb< 0.90] <- 10
final_dataset$ProbCat[final_dataset$excProb>=0.90 & final_dataset$excProb< 1.00] <- 11
final_dataset$ProbCat[final_dataset$excProb == 1.00] <- 12

# check to see if legend scheme is balanced
table(final_dataset$ProbCat)

```

```{r}

# map of exceedance probabilities
tm_shape(final_dataset) + 
    tm_fill("ProbCat", style = "cat", title = "Probability", palette = "GnBu", labels = ProbCategorylist) +
  
     tm_shape(london_boroughs) +
  tm_borders(col = "black", lwd=0.3) +

  ##north arrow
  
  tm_compass(north=0,
             position=c(0.15,0.10),size=1.5, show.labels= 0)+
  
  ##scale bar
  
  tm_scale_bar(position = c(0.003,0.02)) +
  
  ##legend position
  
  tm_layout(legend.position = c(0.84,0.05),
          legend.title.size = 0.76,
          legend.text.size = 0.60) +
  
  ##title
  
   tm_credits("Significance", position=c(0.20,0.94), size=0.8) 

```

